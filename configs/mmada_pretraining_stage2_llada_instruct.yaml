wandb:
  entity: null
  resume: 'auto'

experiment:
    project: "mmada-training-stage2"
    name: "mmada-training-stage2-llada-instruct"
    output_dir: "mmada-training-stage2-llada-instruct"
    max_train_examples_molecular: 1000
    save_every: 10000
    eval_every: 2500
    log_every: 1
    log_grad_norm_every: 100
    resume_from_checkpoint: "latest"

model:
        dataset_type: molecular 
        data_path: "/home/exouser/MMaDA/m3_molecular_data.parquet" # 确保这个路径指向您的实际分子数据文件
        max_selfies_length: 256 
        max_atoms: 256      
        include_edge_bond_dist: True 
        include_rdmol2selfies: False
        selfies_mask_ratio: 0.15

        llm_config_path: "GSAI-ML/LLaDA-8B-Instruct" 
        llm_model_name_or_path: "GSAI-ML/LLaDA-8B-Instruct"
        w_clip_vit: False
        new_vocab_size: 134656
        llm_vocab_size: 126464
        num_new_special_tokens: 0
        tie_word_embeddings: False
        d_model: 4096 

        mol_atom_embedding_dim: 128
        mol_coord_embedding_dim: 64
        mol_3d_encoder_output_dim: 768
        fusion_hidden_dim: 1536
        final_condition_dim: 4096
        num_atom_types: 120
        output_atom_coords_dim: 3
        output_atom_type_dim: 120
        diffusion_timesteps: 1000
        noise_schedule_beta_start: 0.0001
        noise_schedule_beta_end: 0.02
        # --- 添加这一行 ---
        noise_schedule_name: "linear" # 或者 "cosine"，确保与 training.utils.get_noise_schedule 的支持名称一致
        # --- 添加结束 ---

        gradient_checkpointing: True

        coords_coeff: 1.0 
        atom_type_coeff: 1.0 
        selfies_coeff: 0.5 # 确保这个系数非零，以便计算 SELFIES 损失
        alignment_coeff: 0.1 # 确保这个系数非零，以便计算对齐损失
        hierarchical_coeff: 0.0 # 根据您的决定，目前保持为 0.0

        mask_token_id: -1 # train_mmada_stage2.py 中的代码会尝试从 tokenizer 获取正确的值
        mask_replace_ratio: 0.1 
        mask_schedule_name: "linear" # 这是用于离散掩码调度的名称
        mask_schedule_start: 0.0001
        mask_schedule_end: 0.02

dataset:
    params:
        shuffle_buffer_size: 1000
        num_workers: 0
        pin_memory: True
        persistent_workers: False

    preprocessing:
        max_text_length: 512 # for text tokens

optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 5e-5
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.01
        epsilon: 1e-8

lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 5000
        min_lr_scale: 0.1

training:
    gradient_accumulation_steps: 2
    noise_type: "mask"
    batch_size: 2 # 这是 per_device_train_batch_size
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    max_train_steps: 1000000
    overfit_one_batch: False
    cond_dropout_prob: 0.1
    min_masking_rate: 0.0
    label_smoothing: 0.0
    max_grad_norm: 1
    guidance_scale: 3
    generation_timesteps: 12